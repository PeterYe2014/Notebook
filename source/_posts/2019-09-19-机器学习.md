layout: post
title:  "概率论与随机过程" 

---

#Machine Learning

@(机器学习)


##一、Linear Regression 

### 1.1 One variable case

引入问题：我们有一些房子的面积(size)和价格(price)的数据集，通过这样的数据集我们想得到能够预测任何一个size的房子的价格的假设。这里我们通过建立线性拟合模型来解答：

$$h_{\theta }= \theta_{0} + \theta_{1}x$$

同时我们先定义一些符号:

$m：数据实例的个数$
$(x_{i},y_{i})：	表示第 i 个数据集$
$(x,y)：实例集$

现在问题就转化为，寻找恰当的$\theta_{0}$和$\theta_{1}$使得我们的假设和实际数据集的$y$逼近，我们通过误差函数来反映这一逼近程度：

$$J(\theta_0, \theta_1) = \dfrac {1}{2m} \sum _{i=1}^m \left ( \hat{y}_{i}- y_{i} \right)^2 = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2$$

问题进一步转化为优化问题：寻找$\theta_{0}$和$\theta_{1}$来最小化$J(\theta_0,\theta_1)$;

####梯度下降(gradient descent)

想象自己在一个山坡上某一个点，自己要以最快的速度下山，该选择怎么样的路径。
![Alt text](./gradient descend.png)


具体的算法过程如下：

**重复下列过程直到误差函数收敛：**
$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$$
$$\alpha 是学习率，控制\theta_j的变化速度$$

>注意：每次遍历的时候，对于每个参数$\theta$都要同时更新，也就是说，新数据是批量产生的，而不是一个一个产生的。

![Alt text](./simultaneous update.png)

为何梯度下降方法能够起作用呢？我们先看一个简单的单变量问题：

$$\theta_1:=\theta_1-\alpha\frac{d}{d\theta_1}J(\theta_1)……式3$$

这里寻找$\theta_1$使得$J(\theta_1)$最小，下面是两个例子分析了梯度下降的运作过程：

![Alt text](./梯度下降理解.png)

从第一个图看出，选择$\theta_1$初始值在右边，一开始导数值为负，经过迭代，$\theta_1$可以朝着左边移动(也是向着最小值的方向移动的)，而且当有导数为0的时候，$\theta_1$就收敛了。同时当选择初值在右边也会朝着最优化的方向移动。

还有就是学习率$\alpha$的影响，如果学习率过大了之后，向左向右可能会越过最优解，不能收敛，所以要恰当地设置学习率。

从上图可以看出，无论是向左还是向右移动，导数项的绝对值是在减小的，也就是说每次迭代，$\theta_1$移动的步伐会放慢，这样会让其更好地收敛到最优解。

下面我们就可以把误差函数和梯度下降法结合起来分析了，得到一个真正的算法：
$$J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2$$
$$\theta_0 := \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)$$
$$\theta_1 := \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)$$

其中两个导数项分别为：
$$\frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)$$
$$\frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)x_i$$
整理得算法为：

repeat until converge{
$$\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)$$
$$\theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)x_i$$
}

从算法里面看到了求和项，需要遍历所有的实例来进行计算，这种梯度下降也叫“Batch” 梯度下降，计算量很大，后面我们可以继续优化。

###1.2 multiple variables case
根据单变量的情形，很容易推出多变量的线性回归模型，首先建立有k个输入变量的假设模型：

$$h_{\theta}(X^{(j)})=\theta_{0} + x^{(j)}_{1}*\theta_{1} + x^{(j)}_{2}*\theta_{2}+...+x^{(j)}_{k}*\theta_{k}$$

假设我们有输入矩阵$X$所有实例组成的矩阵，定义：

$X^{(j)}：代表第j个实例$
$X^{(j)}_{i}：代表第j个实例中第i个变量的值$
且我们让$X^{(j)}_{1}=1$，即所有实例的第一个变量值为1，则误差函数可以写成：



##二、Logistics Regression 

###2.1 Introduction

有些时候我们想进行一些分类操作，比如垃圾邮件分类，判断图片中物品苹果还是香蕉等等，这个时候我们想要的输出是离散的，之前我们学习的线性回归的输出是连续的，我们也可以将连续的输出分区间来离散化，以此来模拟分类的输出，但是这样并非最好的方法（why）?。
我们考虑分类中最特殊的一种二分类问题，这里提供了另一种回归方法，这种回归的最大特点是输出的值在$[0,1]$区间类，同时具有很好的区分方法，先来一个回归模型:

$$h_{\theta}(x)=\theta^{T}x$$

这个是常规的回归模型，输出是线性的我们再引入一个S函数：

$$g(z) = \dfrac{1}{1+e^{-z}}$$

这个函数如其名，就是一个S型函数，上界为1，下届为0，且$g(0)=0.5$将图像划分为两个半。则可以将$h$函数带入S函数，从而新模型为：

$$h_\theta(x)=\dfrac{1}{1+e^{-\theta^{T}x}}$$

则当$h_\theta >= 0时$为一类，$h_\theta < 0时$为另一类。

但是怎么选择参数$\theta$呢？

###2.2 Cost Function

假设输出二类的$y\in\{0,1\}$，通过最大似然的估计，得出下面的误差函数：

$$J(\theta)=-\dfrac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_{\theta}(x^{(i)})) + (1-y^{(i)}log(1-h_{\theta}(x^{(i)})))]$$

即当$y=0$时，$h(x)=1$误差最大，$y=0$，$h(x)=0$误差为0；

然后通过梯度下降法来得到满足$\min_{\theta}J(\theta)$的$\theta$，同时在线性回归中的feature scaling 在logistics 回归中也有用。

###2.3 Advanced Optimization

在梯度下降的过程中，我们需要自己去设置学习率，并且时刻关注着学习率是否设置的合理，这个是很花费时间的工作，所以，这里有一些其它的优化算法，从而不需要设置学习率，同时收敛速度优于梯度下降方法，并且更适用于大规模的数据：

* Conjugate Gradient
* BFGS
* L-BFGS

这些算法不仅仅自动设置学习率，同时还有一些其它的有点，需要花时间来学习，也不建议自己去实现这些复杂的函数，而是应该去调用一些库文件来实现。

这里我们用Octave来实现一种自动选择学习率的算法，由于不需要设置学习率，我们只要在一个函数中计算误差函数$J(\theta)$的值，同时计算误差函数对于每个参数$\theta_{i}$的偏导数，并且返回，然后再设置一个初始的参数$\theta$，最后把函数和一些优化设置交给一个优化函数就可以了，在Octava中，这个优化函数是*fminunc*，通过help命令可以查看他的帮助:

```help
 -- fminunc (FCN, X0)
 -- fminunc (FCN, X0, OPTIONS)
 -- [X, FVAL, INFO, OUTPUT, GRAD, HESS] = fminunc (FCN, ...)
     Solve an unconstrained optimization problem defined by the function
     FCN. a pointer to function return the objective function value, optionally with gradient.
     X0 determines a starting guess
     OPTIONS. a optimset value,If "GradObj" is "on", it specifies that FCN, when called with two
     On return, X is the location of the minimum and FVAL contains the
   
```
针对只有一个变量的例子：先定义一个函数：
```
function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
end
```
设置优化选项和和初始参数，调用优化函数:
```
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```